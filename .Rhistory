df <- merge(temp_df, discharge_df, on = "Date", how = 'left')
)
fn <- sprintf('%s.csv', ID)
fp <- file.path(SW_out_dam,fn)
write.csv(df, file = fp)
} else { #not enough data or no data available
GAGE_Dam$NWIS_Tavail[n] <-  0
}
View(GAGE_Dam)
GAGE_Dam <- filter(GAGE_loc, (GAGE_loc$RAW_DIS_NEAREST_DAM < dam_dis_max & GAGE_loc$RAW_DIS_NEAREST_DAM >=0))
for (n in nrow(GAGE_Dam)){
ID <- GAGE_Dam$id[n]
df <- data.frame()
temp_df<- renameNWISColumns(readNWISdv(ID, "00010"))#, #Temperature (C)
#startDate = date_se[1],
#endDate = date_se[2]))
df <- temp_df
if (nrow(df)>(0.75*365)){# at least has 75% of one year worth of temp data
GAGE_Dam$NWIS_Tavail[n] <-  1
try(
df <- temp_df # if no discharge data is available temp will still be downloaded)
)
# also download discharge data
try( #will still have temperature data even if no available
discharge_df<- readNWISdv(ID, "00060"),#, #Daily Discharge cubic feet per second
#startDate = date_se[1],
#endDate = date_se[2]))
df <- merge(temp_df, discharge_df, on = "Date", how = 'left')
)
fn <- sprintf('%s.csv', ID)
fp <- file.path(SW_out_dam,fn)
write.csv(df, file = fp)
} else { #not enough data or no data available
GAGE_Dam$NWIS_Tavail[n] <-  0
}
}
#
View(GAGE_Dam)
for (n in nrow(GAGE_Dam)){
ID <- GAGE_Dam$id[n]
df <- data.frame()
temp_df<- renameNWISColumns(readNWISdv(ID, "00010"))#, #Temperature (C)
#startDate = date_se[1],
#endDate = date_se[2]))
df <- temp_df
if (nrow(df)>(0.75*365)){# at least has 75% of one year worth of temp data
GAGE_Dam$NWIS_Tavail[n] <-  1
tryCatch(
df <- temp_df # if no discharge data is available temp will still be downloaded)
)
# also download discharge data
tryCatch( #will still have temperature data even if no available
discharge_df<- readNWISdv(ID, "00060"),#, #Daily Discharge cubic feet per second
#startDate = date_se[1],
#endDate = date_se[2]))
df <- merge(temp_df, discharge_df, on = "Date", how = 'left')
)
fn <- sprintf('%s.csv', ID)
fp <- file.path(SW_out_dam,fn)
write.csv(df, file = fp)
} else { #not enough data or no data available
GAGE_Dam$NWIS_Tavail[n] <-  0
}
}
ID <- GAGE_Dam$id[n]
df <- data.frame()
temp_df<- renameNWISColumns(readNWISdv(ID, "00010"))#, #Temperature (C)
#startDate = date_se[1],
#endDate = date_se[2]))
df <- temp_df
##--- Library
library(dataRetrieval) #Retreive NWIS datasetset
library(dplyr)#for filter
temp_df<- renameNWISColumns(readNWISdv(ID, "00010"))
ID <- "01463500"
temp_df<- renameNWISColumns(readNWISdv(ID, "00010"))#, #Temperature (C)
#startDate = date_se[1],
#endDate = date_se[2]))
df <- temp_df
if (nrow(df)>(0.75*365)){# at least has 75% of one year worth of temp data
GAGE_Dam$NWIS_Tavail[n] <-  1
tryCatch(
df <- temp_df # if no discharge data is available temp will still be downloaded)
)
# also download discharge data
tryCatch( #will still have temperature data even if no available
discharge_df<- readNWISdv(ID, "00060"),#, #Daily Discharge cubic feet per second
#startDate = date_se[1],
#endDate = date_se[2]))
df <- merge(temp_df, discharge_df, on = "Date", how = 'left')
)
fn <- sprintf('%s.csv', ID)
fp <- file.path(SW_out_dam,fn)
write.csv(df, file = fp)
}
discharge_df<- readNWISdv(ID, "00060")
df <- merge(temp_df, discharge_df, on = "Date", how = 'left')
fn <- sprintf('%s.csv', ID)
fp <- file.path(SW_out_dam,fn)
write.csv(df, file = fp)
0.75*365
nrow(df)>(0.75*365)
ID <- "01463500"
temp_df<- renameNWISColumns(readNWISdv(ID, "00010"))#, #Temperature (C)
#startDate = date_se[1],
#endDate = date_se[2]))
df <- temp_df
if (nrow(df)>(0.75*365)){# at least has 75% of one year worth of temp data
GAGE_Dam$NWIS_Tavail[n] <-  1
try(
df <- temp_df # if no discharge data is available temp will still be downloaded)
)
#--- also download discharge data if available
try(
discharge_df<- renameNWISColumns(readNWISdv(ID, "00060")),#, #Daily Discharge cubic feet per second
#startDate = date_se[1],
#endDate = date_se[2]))
df <- merge(temp_df, discharge_df, on = "Date", how = 'left')
)
fn <- sprintf('%s.csv', ID)
fp <- file.path(SW_out_dam,fn)
write.csv(df, file = fp)
} else { #not enough data or no data available
GAGE_Dam$NWIS_Tavail[n] <-  0
}
ID <- GAGE_Dam$id[n]
for (n in nrow(GAGE_Dam)){
ID <- GAGE_Dam$id[n]
#ID <- "01463500" trial run
temp_df<- renameNWISColumns(readNWISdv(ID, "00010"))#, #Temperature (C)
#startDate = date_se[1],
#endDate = date_se[2]))
df <- temp_df
if (nrow(df)>(0.75*365)){# at least has 75% of one year worth of temp data
GAGE_Dam$NWIS_Tavail[n] <-  1
try(
df <- temp_df # if no discharge data is available temp will still be downloaded)
)
#--- also download discharge data if available
try(
discharge_df<- renameNWISColumns(readNWISdv(ID, "00060")),#, #Daily Discharge cubic feet per second
#startDate = date_se[1],
#endDate = date_se[2]))
df <- merge(temp_df, discharge_df, on = "Date", how = 'left')
)
fn <- sprintf('%s.csv', ID)
fp <- file.path(SW_out_dam,fn)
write.csv(df, file = fp)
} else { #not enough data or no data available
GAGE_Dam$NWIS_Tavail[n] <-  0
}
}
#
try(
discharge_df<- renameNWISColumns(readNWISdv(ID, "00060")),#, #Daily Discharge cubic feet per second
#startDate = date_se[1],
#endDate = date_se[2]))
df <- merge(temp_df, discharge_df, on = "Date", how = 'left')
)
for (n in nrow(GAGE_Dam)){
ID <- GAGE_Dam$id[n]
#ID <- "01463500" trial run
temp_df<- renameNWISColumns(readNWISdv(ID, "00010"))#, #Temperature (C)
#startDate = date_se[1],
#endDate = date_se[2]))
df <- temp_df
if (nrow(df)>(0.75*365)){# at least has 75% of one year worth of temp data
GAGE_Dam$NWIS_Tavail[n] <-  1
try( {
df <- temp_df # if no discharge data is available temp will still be downloaded)
}
)
#--- also download discharge data if available
try({
discharge_df<- renameNWISColumns(readNWISdv(ID, "00060"))#, #Daily Discharge cubic feet per second
#startDate = date_se[1],
#endDate = date_se[2]))
df <- merge(temp_df, discharge_df, on = "Date", how = 'left')
}
)
fn <- sprintf('%s.csv', ID)
fp <- file.path(SW_out_dam,fn)
write.csv(df, file = fp)
} else { #not enough data or no data available
GAGE_Dam$NWIS_Tavail[n] <-  0
}
}
#
for (n in nrow(GAGE_Dam)){
ID <- GAGE_Dam$id[n]
#ID <- "01463500" trial run
temp_df<- renameNWISColumns(readNWISdv(ID, "00010"))#, #Temperature (C)
#startDate = date_se[1],
#endDate = date_se[2]))
df <- temp_df
if (nrow(df)>(0.75*365)){# at least has 75% of one year worth of temp data
GAGE_Dam$NWIS_Tavail[n] <-  1
#--- also download discharge data if available
try({
discharge_df<- renameNWISColumns(readNWISdv(ID, "00060"))#, #Daily Discharge cubic feet per second
#startDate = date_se[1],
#endDate = date_se[2]))
df <- merge(temp_df, discharge_df, on = "Date", how = 'left')
}
)
fn <- sprintf('%s.csv', ID)
fp <- file.path(SW_out_dam,fn)
write.csv(df, file = fp)
} else { #not enough data or no data available
GAGE_Dam$NWIS_Tavail[n] <-  0
}
}
#
n <- 100
ID <- GAGE_Dam$id[n]
#ID <- "01463500" trial run
temp_df<- renameNWISColumns(readNWISdv(ID, "00010"))#, #Temperature (C)
#startDate = date_se[1],
#endDate = date_se[2]))
df <- temp_df
if (nrow(df)>(0.75*365)){# at least has 75% of one year worth of temp data
GAGE_Dam$NWIS_Tavail[n] <-  1
#--- also download discharge data if available
try({
discharge_df<- renameNWISColumns(readNWISdv(ID, "00060"))#, #Daily Discharge cubic feet per second
#startDate = date_se[1],
#endDate = date_se[2]))
df <- merge(temp_df, discharge_df, on = "Date", how = 'left')
}
)
fn <- sprintf('%s.csv', ID)
fp <- file.path(SW_out_dam,fn)
write.csv(df, file = fp)
} else { #not enough data or no data available
GAGE_Dam$NWIS_Tavail[n] <-  0
}
nrow(GAGE_Dam)
for (n in nrow(GAGE_Dam)){
ID <- GAGE_Dam$id[n]
print(ID)}
for (n in 1:nrow(GAGE_Dam)){
ID <- GAGE_Dam$id[n]
#ID <- "01463500" trial run
temp_df<- renameNWISColumns(readNWISdv(ID, "00010"))#, #Temperature (C)
df <- temp_df
if (nrow(df)>(0.75*365)){# at least has 75% of one year worth of temp data
GAGE_Dam$NWIS_Tavail[n] <-  1
#--- also download discharge data if available
try({
discharge_df<- renameNWISColumns(readNWISdv(ID, "00060"))#, #Daily Discharge cubic feet per second
#startDate = date_se[1],
#endDate = date_se[2]))
df <- merge(temp_df, discharge_df, on = "Date", how = 'left')
}
)
fn <- sprintf('%s.csv', ID)
fp <- file.path(SW_out_dam,fn)
write.csv(df, file = fp)
} else { #not enough data or no data available
GAGE_Dam$NWIS_Tavail[n] <-  0
}
}
#
GAGE_Dam <- filter(GAGE_Dam, NWIS_Tavail== 1)
i <- a
i <- 'a'
fn <- sprintf('%s.csv', i)
fp <- file.path('input/AR_NOAA',fn)
fp <- file.path('input/NOAA_pullR',fn)
## Get NOAA DATA
#Get a list of all the NOAA stations currently available
#G_st <- ghcnd_stations() #Use first time, or if need of an update - takes a long time.
load("input/NOAA_Stations_All.RData")#Already loaded all NOAA station data as G_st
# Determine the nearest NOAA station to each of the SW station in the list filter for only SW stations that meet the timeframe requirements
# Here we look for the NOAA station within 25 mi of station, no data returned for SW stations
# without any within that range. Limit = 1 only provide 1 per station (the closet one)
nearby_stations <- meteo_nearby_stations(lat_lon_df = GAGE_Dam,
lat_colname = "LAT_GAGE", # "dec_lat_va",#
lon_colname = "LNG_GAGE", #" #"dec_long_va",
station_data = G_st,
#year_min = 2011,#This way we wont get air temp records with just 2010, but include 2010 in data pull- see below
#year_max = 2018,
var = c("TMAX", "TMIN", "TAVG"),
radius = 25,
limit = 1)
# Merge the nearby station data to one dataframe
match_data = do.call(rbind, nearby_stations) # make output of meteo nearby stations to a usable df
# Make a list of the unique NOAA station ids (remove duplicates)
noaa_pull <- unique(match_data$id)
# remove NA values (if some stations do not have lat long this will occur)
noaa_pull <- noaa_pull[!is.na(noaa_pull)]
#put in question to pull air data
# Pull the data for each of the NOAA stations identified as closest stations to the input SW stations
for (i in noaa_pull) {
fn <- sprintf('%s.csv', i)
fp <- file.path('input/NOAA_pullR',fn)
# If the NOAA station is not already downloaded, download it. This is so i dont have to manipulate input files for speed
if(!file.exists(fp)){
# If there is an error the script will continue to run
tryCatch({
# Pull the station temperature data
df <- meteo_pull_monitors(monitors = i,
var = c("TAVG", "TMIN", "TMAX"))
rec <- colnames(df[3:length(df)])
for (n in rec){
df_temp <- df[n]/10
df[n] <- df_temp
}
write.csv(df, file = fp)
},
error=function(e){cat("ERROR :",conditionMessage(e), "\n")})
}
}
#change index/rownames to be a active column - locname is consistent for next step (2)
match_data$locname <- row.names(match_data)
#change original input siteno/id to locname for match (needed to be id for last package to work)
colnames(lat_lon_SW_df)[colnames(lat_lon_SW_df)=="id"] <- "locname"
#Change Column names to match input for step 2, as well as provide appropriate colnames for join with SW location data
colnames(match_data)[names(match_data) == "id"] <- "NOAA_ID"
colnames(match_data)[names(match_data) == "name"] <- "NOAA_NAME"
colnames(match_data)[names(match_data) == "latitude"] <- "NEAR_Y"
colnames(match_data)[names(match_data) == "longitude"] <- "NEAR_X"
colnames(match_data)[names(match_data) == "distance"] <- "NEAR_DIST"
#remove geometry columns if converted from UTM
lat_lon_SW_df <- select(lat_lon_SW_df,locname, latitude, longitude)
# Merge the SW and Air Station data together (in the same format as the ArcGIS python script)
station_loc <- left_join(lat_lon_SW_df,match_data)
#Export this merge data set for next step (step 2 in python for annual signal)
write.csv(station_loc, file = f_out)
## Get NOAA DATA
#Get a list of all the NOAA stations currently available
#G_st <- ghcnd_stations() #Use first time, or if need of an update - takes a long time.
load("input/NOAA_Stations_All.RData")#Already loaded all NOAA station data as G_st
# Determine the nearest NOAA station to each of the SW station in the list filter for only SW stations that meet the timeframe requirements
# Here we look for the NOAA station within 25 mi of station, no data returned for SW stations
# without any within that range. Limit = 1 only provide 1 per station (the closet one)
nearby_stations <- meteo_nearby_stations(lat_lon_df = GAGE_Dam,
lat_colname = "LAT_GAGE", # "dec_lat_va",#
lon_colname = "LNG_GAGE", #" #"dec_long_va",
station_data = G_st,
#year_min = 2011,#This way we wont get air temp records with just 2010, but include 2010 in data pull- see below
#year_max = 2018,
var = c("TMAX", "TMIN", "TAVG"),
radius = 25,
limit = 1)
# Merge the nearby station data to one dataframe
match_data = do.call(rbind, nearby_stations) # make output of meteo nearby stations to a usable df
# Make a list of the unique NOAA station ids (remove duplicates)
noaa_pull <- unique(match_data$id)
# remove NA values (if some stations do not have lat long this will occur)
noaa_pull <- noaa_pull[!is.na(noaa_pull)]
#put in question to pull air data
# Pull the data for each of the NOAA stations identified as closest stations to the input SW stations
for (i in noaa_pull) {
fn <- sprintf('%s.csv', i)
fp <- file.path('input/NOAA_pullR',fn)
# If the NOAA station is not already downloaded, download it. This is so i dont have to manipulate input files for speed
if(!file.exists(fp)){
# If there is an error the script will continue to run
tryCatch({
# Pull the station temperature data
df <- meteo_pull_monitors(monitors = i,
var = c("TAVG", "TMIN", "TMAX"))
rec <- colnames(df[3:length(df)])
for (n in rec){
df_temp <- df[n]/10
df[n] <- df_temp
}
write.csv(df, file = fp)
},
error=function(e){cat("ERROR :",conditionMessage(e), "\n")})
}
}
#change index/rownames to be a active column - locname is consistent for next step (2)
match_data$locname <- row.names(match_data)
#change original input siteno/id to locname for match (needed to be id for last package to work)
colnames(GAGE_Dam)[colnames(GAGE_Dam)=="id"] <- "locname"
#Change Column names to match input for step 2, as well as provide appropriate colnames for join with SW location data
colnames(match_data)[names(match_data) == "id"] <- "NOAA_ID"
colnames(match_data)[names(match_data) == "name"] <- "NOAA_NAME"
colnames(match_data)[names(match_data) == "latitude"] <- "NEAR_Y"
colnames(match_data)[names(match_data) == "longitude"] <- "NEAR_X"
colnames(match_data)[names(match_data) == "distance"] <- "NEAR_DIST"
#remove geometry columns if converted from UTM
GAGE_Dam <- select(GAGE_Dam,locname, latitude, longitude)
# Merge the SW and Air Station data together (in the same format as the ArcGIS python script)
station_loc <- left_join(GAGE_Dam,match_data)
#Export this merge data set for next step (step 2 in python for annual signal)
write.csv(station_loc, file = f_out)
library(rnoaa)
nearby_stations <- meteo_nearby_stations(lat_lon_df = GAGE_Dam,
lat_colname = "LAT_GAGE", # "dec_lat_va",#
lon_colname = "LNG_GAGE", #" #"dec_long_va",
station_data = G_st,
#year_min = 2011,#This way we wont get air temp records with just 2010, but include 2010 in data pull- see below
#year_max = 2018,
var = c("TMAX", "TMIN", "TAVG"),
radius = 25,
limit = 1)
# Merge the nearby station data to one dataframe
match_data = do.call(rbind, nearby_stations) # make output of meteo nearby stations to a usable df
# Make a list of the unique NOAA station ids (remove duplicates)
noaa_pull <- unique(match_data$id)
# remove NA values (if some stations do not have lat long this will occur)
noaa_pull <- noaa_pull[!is.na(noaa_pull)]
#put in question to pull air data
# Pull the data for each of the NOAA stations identified as closest stations to the input SW stations
for (i in noaa_pull) {
fn <- sprintf('%s.csv', i)
fp <- file.path('input/NOAA_pullR',fn)
# If the NOAA station is not already downloaded, download it. This is so i dont have to manipulate input files for speed
if(!file.exists(fp)){
# If there is an error the script will continue to run
tryCatch({
# Pull the station temperature data
df <- meteo_pull_monitors(monitors = i,
var = c("TAVG", "TMIN", "TMAX"))
rec <- colnames(df[3:length(df)])
for (n in rec){
df_temp <- df[n]/10
df[n] <- df_temp
}
write.csv(df, file = fp)
},
error=function(e){cat("ERROR :",conditionMessage(e), "\n")})
}
}
#change index/rownames to be a active column - locname is consistent for next step (2)
match_data$locname <- row.names(match_data)
#change original input siteno/id to locname for match (needed to be id for last package to work)
colnames(GAGE_Dam)[colnames(GAGE_Dam)=="id"] <- "locname"
#Change Column names to match input for step 2, as well as provide appropriate colnames for join with SW location data
colnames(match_data)[names(match_data) == "id"] <- "NOAA_ID"
colnames(match_data)[names(match_data) == "name"] <- "NOAA_NAME"
colnames(match_data)[names(match_data) == "latitude"] <- "NEAR_Y"
colnames(match_data)[names(match_data) == "longitude"] <- "NEAR_X"
colnames(match_data)[names(match_data) == "distance"] <- "NEAR_DIST"
#remove geometry columns if converted from UTM
GAGE_Dam <- select(GAGE_Dam,locname, latitude, longitude)
# Merge the SW and Air Station data together (in the same format as the ArcGIS python script)
station_loc <- left_join(GAGE_Dam,match_data)
#Export this merge data set for next step (step 2 in python for annual signal)
write.csv(station_loc, file = f_out)
nearby_stations <- meteo_nearby_stations(lat_lon_df = GAGE_Dam,
lat_colname = "LAT_GAGE", # "dec_lat_va",#
lon_colname = "LNG_GAGE", #" #"dec_long_va",
station_data = G_st,
#year_min = 2011,#This way we wont get air temp records with just 2010, but include 2010 in data pull- see below
#year_max = 2018,
var = c("TMAX", "TMIN", "TAVG"),
radius = 25,
limit = 1)
GAGE_Dam$id <- GAGE_Dam$locname
nearby_stations <- meteo_nearby_stations(lat_lon_df = GAGE_Dam,
lat_colname = "LAT_GAGE", # "dec_lat_va",#
lon_colname = "LNG_GAGE", #" #"dec_long_va",
station_data = G_st,
#year_min = 2011,#This way we wont get air temp records with just 2010, but include 2010 in data pull- see below
#year_max = 2018,
var = c("TMAX", "TMIN", "TAVG"),
radius = 25,
limit = 1)
match_data = do.call(rbind, nearby_stations) # make output of meteo nearby stations to a usable df
# Make a list of the unique NOAA station ids (remove duplicates)
noaa_pull <- unique(match_data$id)
# remove NA values (if some stations do not have lat long this will occur)
noaa_pull <- noaa_pull[!is.na(noaa_pull)]
for (i in noaa_pull) {
fn <- sprintf('%s.csv', i)
fp <- file.path('input/NOAA_pullR',fn)
# If the NOAA station is not already downloaded, download it. This is so i dont have to manipulate input files for speed
if(!file.exists(fp)){
# If there is an error the script will continue to run
tryCatch({
# Pull the station temperature data
df <- meteo_pull_monitors(monitors = i,
var = c("TAVG", "TMIN", "TMAX"))
rec <- colnames(df[3:length(df)])
for (n in rec){
df_temp <- df[n]/10
df[n] <- df_temp
}
write.csv(df, file = fp)
},
error=function(e){cat("ERROR :",conditionMessage(e), "\n")})
}
}
#change index/rownames to be a active column - locname is consistent for next step (2)
match_data$locname <- row.names(match_data)
#change original input siteno/id to locname for match (needed to be id for last package to work)
colnames(GAGE_Dam)[colnames(GAGE_Dam)=="id"] <- "locname"
#Change Column names to match input for step 2, as well as provide appropriate colnames for join with SW location data
colnames(match_data)[names(match_data) == "id"] <- "NOAA_ID"
colnames(match_data)[names(match_data) == "name"] <- "NOAA_NAME"
colnames(match_data)[names(match_data) == "latitude"] <- "NEAR_Y"
colnames(match_data)[names(match_data) == "longitude"] <- "NEAR_X"
colnames(match_data)[names(match_data) == "distance"] <- "NEAR_DIST"
#remove geometry columns if converted from UTM
GAGE_Dam <- select(GAGE_Dam,locname, latitude, longitude)
# Merge the SW and Air Station data together (in the same format as the ArcGIS python script)
station_loc <- left_join(GAGE_Dam,match_data)
#Export this merge data set for next step (step 2 in python for annual signal)
write.csv(station_loc, file = f_out)
station_loc <- left_join(GAGE_Dam,match_data, by = "locname")
View(GAGE_Dam)
GAGE_Dam <- GAGE_Dam[, !duplicated(colnames(GAGE_Dam))]#remove duplicate column names locname
station_loc <- left_join(GAGE_Dam,match_data, by = "locname")
write.csv(station_loc, file = f_out)
f_out <- "input"
write.csv(station_loc, file = f_out)
f_out <- "input/GAGE_NWIS_NOAA_LocData.csv"
write.csv(station_loc, file = f_out)
